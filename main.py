from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse
import json
import time
import os

import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

# ğŸ“Œ CrÃ©er le dossier pour stocker les fichiers
DATA_FOLDER = "data"
os.makedirs(DATA_FOLDER, exist_ok=True)

app = FastAPI()

# ğŸ“Œ Route de test pour voir si l'API fonctionne
@app.get("/")
def home():
    return {"message": "API de Scraping en ligne ğŸš€"}

# ğŸ“Œ Fonction pour initialiser un WebDriver propre Ã  chaque requÃªte
def init_driver():
    """Initialise un WebDriver Chrome propre pour Ã©viter les crashes."""
    chromedriver_autoinstaller.install()

    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--headless")  # Mode headless (pas d'interface graphique)
    chrome_options.add_argument("--no-sandbox")  # NÃ©cessaire pour Railway
    chrome_options.add_argument("--disable-dev-shm-usage")  # Ã‰vite les erreurs mÃ©moire
    chrome_options.add_argument("--disable-gpu")  # DÃ©sactive le GPU
    chrome_options.add_argument("--remote-debugging-port=9222")  # Debugging

    return webdriver.Chrome(options=chrome_options)

# ğŸ“Œ Route API pour scraper un site (Exemple avec Bienici)
@app.get("/scrape")
def scrape():
    """Scrape des annonces immobiliÃ¨res et retourne un fichier JSON."""
    base_url = "https://www.bienici.com/recherche/achat/saint-quentin-02100?page="
    page_number = 1
    annonces_data = []

    driver = init_driver()  # ğŸ”¥ CrÃ©er un WebDriver pour cette requÃªte

    while True:
        url = base_url + str(page_number)
        print(f"\nğŸ” Scraping de la page {page_number} : {url}")

        driver.get(url)
        time.sleep(5)  # Pause pour charger JS

        # âœ… Trouver les annonces
        annonces = driver.find_elements(By.CSS_SELECTOR, "a.detailedSheetLink")
        if len(annonces) == 0:
            print(f"âŒ Aucune annonce trouvÃ©e sur la page {page_number}. ArrÃªt du scraping.")
            break

        # âœ… Extraire les URLs
        annonce_urls = [a.get_attribute("href") for a in annonces]
        print(f"âœ… {len(annonce_urls)} annonces trouvÃ©es sur la page {page_number}")

        # ğŸ”¥ Scraper les dÃ©tails de chaque annonce
        for annonce_url in annonce_urls:
            print(f"ğŸ” Scraping dÃ©tails : {annonce_url}")
            driver.get(annonce_url)
            time.sleep(5)  # Pause pour charger la page

            try:
                titre = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
                prix = driver.find_element(By.CSS_SELECTOR, ".ad-price__the-price").text.strip()
                description = driver.find_element(By.CSS_SELECTOR, ".see-more-description__content").text.strip()

                # âœ… Ajouter les donnÃ©es
                annonces_data.append({
                    "URL": annonce_url,
                    "Titre": titre,
                    "Prix": prix,
                    "Description": description
                })

            except Exception as e:
                print(f"âŒ Erreur sur {annonce_url} : {e}")

        # âœ… Passer Ã  la page suivante
        page_number += 1
        time.sleep(3)

    driver.quit()  # ğŸ”¥ Fermer WebDriver aprÃ¨s la requÃªte

    # âœ… Sauvegarde en JSON
    output_file = f"{DATA_FOLDER}/bienici_data.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(annonces_data, f, indent=4, ensure_ascii=False)

    print(f"\nâœ… DonnÃ©es enregistrÃ©es dans {output_file}")

    return FileResponse(output_file, media_type="application/json", filename="bienici_data.json")

